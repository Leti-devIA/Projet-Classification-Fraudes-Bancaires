import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.over_sampling import SMOTEN
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from imblearn.combine import SMOTEENN, SMOTETomek
from lightgbm import LGBMClassifier
from sklearn.ensemble import GradientBoostingClassifier



# ------------------------------------------- FONCTIONS ------------------------------------------- #

# Fonction pour appliquer diff√©rentes techniques de r√©√©quilibrage des classes
def fonction_sampling(X_train, y_train, sampling_technique='None'):
    """
    Applique diff√©rentes techniques de r√©√©chantillonnage pour √©quilibrer les classes.
    
    :param X_train: Donn√©es d'entra√Ænement (features)
    :param y_train: Labels des donn√©es d'entra√Ænement
    :param sampling_technique: Technique d'√©chantillonnage √† appliquer
    :return: Donn√©es r√©√©chantillonn√©es (X_train_res, y_train_res)
    
    """
    if sampling_technique == 'SMOTE':
        smote = SMOTE(random_state=42)
        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
    elif sampling_technique == 'NearMiss':
        nearmiss = NearMiss()
        X_train_res, y_train_res = nearmiss.fit_resample(X_train, y_train)
    elif sampling_technique == 'SMOTEN':
        smoten = SMOTEN()
        X_train_res, y_train_res = smoten.fit_resample(X_train, y_train)
    elif sampling_technique == "SMOTETomek":
        smotetomek = SMOTETomek()
        X_train_res, y_train_res = smotetomek.fit_resample(X_train, y_train)
    elif sampling_technique == 'SMOTEENN':
        smoteenn = SMOTEENN()
        X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)
    else:
        # Aucun r√©√©chantillonnage appliqu√©
        X_train_res, y_train_res = X_train, y_train
        
    return X_train_res, y_train_res



# Fonction pour tester diff√©rents mod√®les
def tester_modele(modele, X_train, y_train, X_test, y_test, sampling):
    """
    Entra√Æne et √©value un mod√®le avec une technique d'√©chantillonnage donn√©e.
    
    :param modele: Mod√®le de machine learning √† tester
    :param X_train: Donn√©es d'entra√Ænement
    :param y_train: Labels d'entra√Ænement
    :param X_test: Donn√©es de test
    :param y_test: Labels de test
    :param sampling: Technique d'√©chantillonnage utilis√©e
    
    """
    # Impressions pour savoir o√π en est le test 
    print(f"\nüîç Test du mod√®le : {modele.__class__.__name__} avec {sampling}")
    modele.fit(X_train, y_train)  # Entra√Ænement du mod√®le
    y_pred = modele.predict(X_test)  # Pr√©dictions

    # Affichage des m√©triques de performance
    accuracy = accuracy_score(y_test, y_pred)
    print(f"‚úÖ Accuracy: {accuracy:.4f}")
    print("\nüìä Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # Matrice de confusion
    conf_matrix = confusion_matrix(y_test, y_pred)
    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]

    # Affichage de la matrice de confusion
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues', 
                xticklabels=['Non Fraude', 'Fraude'], 
                yticklabels=['Non Fraude', 'Fraude'])
    plt.xlabel("Pr√©dictions")
    plt.ylabel("Vraies valeurs")
    plt.title(f"Matrice de Confusion - {modele.__class__.__name__} ({sampling})")
    plt.show()


# ------------------------------------------- FICHIER ------------------------------------------- #

# Ouverture du fichier 
df = pd.read_csv("ETL_csv.csv")

# ------------------------------------------- NOUVELLES COLONNES -------------------------------------------#

# Transformation des noms des colonnes
df.columns = ['id_trans', 'etape', 'type', 'montant', 'id_orig', 'old_solde_exp',
              'new_solde_exp', 'id_dest', 'old_solde_dest', 'new_solde_dest',
              'fraude']

# Cr√©ation de nouvelles features 
# Cr√©ation de la colonne qui fait la diff√©rence sur le solde de l'exp√©diteur
df['diff_solde_orig'] = abs(df['old_solde_exp'] - df['new_solde_exp'])

# Cr√©ation de la colonne qui fait la diff√©rence sur le solde du destinataire
df['diff_solde_dest'] = abs(df['old_solde_dest'] - df['new_solde_dest'])

# Ne garder que les colonnes qu'on juge utiles
df = df[['type', 'montant', 'diff_solde_orig', 'diff_solde_dest', 'fraude']]

print(df.head())

# ------------------------------------------- NORMALISATION ------------------------------------------- #

# D√©finir les colonnes num√©riques et cat√©gorielles
num_features = ['montant', 'diff_solde_orig', 'diff_solde_dest']  # Ajoutez ici vos colonnes num√©riques
cat_features = ['type']  # Ajoutez ici vos colonnes cat√©gorielles

# Sur donn√©es num√©riques
scaler = StandardScaler()
df[num_features] = scaler.fit_transform(df[num_features])

#Sur donn√©es cat√©gorielles
encodage = OneHotEncoder(sparse_output=False)  # `sparse=False` pour obtenir un tableau dense
encoded_type = encodage.fit_transform(df[['type']])

# Convertir l'array en DataFrame et ajouter les colonnes encod√©es √† df
df_encoded = pd.DataFrame(encoded_type, columns=encodage.get_feature_names_out(['type']))
df = pd.concat([df, df_encoded], axis=1)

# Suppression de la colonne 'type' initiale
df = df.drop(['type'], axis=1)

# Encodage de la colonne 'type' en valeurs num√©riques (uniquement utilis√© pour le RandomForest)
# label_encoder = LabelEncoder()
# df['type'] = label_encoder.fit_transform(df['type'])

print(df.head())


# ------------------------------------------- TESTS ------------------------------------------- #

# Diviser les donn√©es d'entrainement
X = df.drop('fraude', axis=1)
y = df['fraude']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Liste des mod√®les √† tester
modeles = [
    # LogisticRegression(),
    # XGBClassifier(),
    # GaussianNB(),
    # KNeighborsClassifier(), 
    # RandomForestClassifier(),
    # LGBMClassifier(),
    GradientBoostingClassifier()

]

# Liste des techniques de r√©√©chantillonnage
liste_techniques = ["SMOTE", "SMOTETomek"]

# Boucle pour tester chaque mod√®le avec chaque technique d'√©chantillonnage
for sampling in liste_techniques:
    X_train_res, y_train_res = fonction_sampling(X_train, y_train, sampling)
    for modele in modeles:
        tester_modele(modele, X_train_res, y_train_res, X_test, y_test, sampling)
